# The list of artifacts to be deployed with each release.
# {version} is substituted with the version being released in path and name.
artifacts:
  - name: cdf-s3-replicator-{version}.tar.gz
    path: cdf-s3-replicator-{version}-linux.tar.gz
    platform: linux
    displayName: "Linux executable"
  - name: cdf-s3-replicator-{version}-docker.tar
    path: cdf-s3-replicator-{version}-docker.tar
    platform: docker
    displayName: "Docker image"

# A list of releases, when running the release script only the entry matching the
# provided version is used.
versions:
  "1.0.0":
    description: "Major release - AWS S3 Data Modeling Replicator"
    changelog:
      added:
        - "Complete refactor from Microsoft Fabric to AWS S3"
        - "Focus exclusively on Data Modeling replication"
        - "Replaced explicit AWS credentials with IAM role support"
        - "Updated to use Pulumi for AWS infrastructure deployment"
        - "Automatic environment detection for AWS (ECS/EC2)"
        - "Explicit data model versioning in S3 paths"
        - "Automatic cursor expiration recovery"
        - "Tableau-optimized publish layer with atomic file replacement"
        - "Support for BI tool"
        - "CloudWatch integration for monitoring"
        - "ECS Fargate deployment option"
        - "Schema evolution handling for Tableau compatibility"
        - "Memory optimization for large data models"
        - "Delta Lake write performance"

# Extractor definition
extractor:
  externalId: cdf-s3-data-modeling-replicator
  name: CDF S3 Data Modeling Replicator
  description: Stream CDF Data Modeling instances to AWS S3 in Delta Lake format for analytics.
  documentation: |
    The CDF S3 Data Modeling Replicator streams data from Cognite Data Fusion (CDF) Data Modeling
    instances to Amazon S3 in Delta Lake format, optimized for analytics and BI tools.

    ## Key Features:
    * **Incremental Sync** - Efficiently syncs only changed data using cursor-based change detection
    * **Automatic Recovery** - Handles cursor expiration and connection issues gracefully
    * **Versioned Models** - Explicit data model versioning with organized S3 paths
    * **BI-Optimized** - Publish layer with stable Parquet files for Tableau, Power BI, and Statistica
    * **Schema Evolution** - Automatic handling of schema changes without data loss
    * **AWS Native** - Uses IAM roles for secure credential-free S3 access
    * **Remote Configuration** - Configure via CDF extraction pipelines

    ## Architecture:
    The replicator creates two data layers in S3:
    * **Raw Layer** (Delta Lake) - Incremental updates with full history at `raw/<space>/<model>/<version>/views/`
    * **Publish Layer** (Parquet) - Deduplicated snapshots for BI at `publish/<space>/<model>/<version>/<view>/`

    ## Deployment:
    * **Local** - Run with Poetry or Docker for development
    * **AWS** - Deploy to ECS Fargate with Pulumi infrastructure-as-code
    * **Authentication** - Automatic IAM role detection in AWS, explicit credentials for local development

    ## Supported BI Tools:
    * Tableau Desktop
    * Power BI
    * Databricks
    * Amazon Athena
    * Apache Spark
    * Snowflake
    * Statistica
  type: global
  tags:
    - aws
    - s3
    - delta-lake
    - data-modeling
    - bi
    - analytics
    - tableau
    - parquet
  image: assets/s3.svg

schema:
  rootSchema: schema/config.schema.json

documentation:
  includeLeafNodes: false
  preamble:
    - docs_preamble.md
    - https://raw.githubusercontent.com/cognitedata/python-extractor-utils/318d74db49536baabb707b2587e1244037e9cc07/schema/docs/remote_tip.md
    - https://raw.githubusercontent.com/cognitedata/python-extractor-utils/318d74db49536baabb707b2587e1244037e9cc07/schema/docs/envsub.md
  skipHeader: true
